{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c73056",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "- Sequencing \n",
    "- Padding\n",
    "- Stemming\n",
    "- Lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad06f5f",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63fa7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd2e49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning': 1, 'we': 2, 'love': 3, 'machine': 4, 'and': 5, 'deep': 6}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=[\"We love machine learning and deep learning\"]\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dfb6c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning': 1, 'machine': 2, 'and': 3, 'deep': 4, 'we': 5, 'love': 6}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization is not case sensitive\n",
    "# Keras Tokenizer class removes special Characters\n",
    "\n",
    "sentence=[\"We love @ machine learning $ and DeeP....LearNing ,.. MACHINE and DEEP learning\"]\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08748d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'we': 1,\n",
       " 'sequencing': 2,\n",
       " 'are': 3,\n",
       " 'learning': 4,\n",
       " 'tokenization': 5,\n",
       " 'next': 6,\n",
       " 'will': 7,\n",
       " 'learn': 8,\n",
       " 'then': 9,\n",
       " 'stemming': 10,\n",
       " 'and': 11,\n",
       " 'lemmetization': 12}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=[\"We are learning tokenization\",\"next we will learn sequencing\",\"then Sequencing , Stemming and Lemmetization\"]\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef9fc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'we': 1,\n",
       " 'sequencing': 2,\n",
       " '5': 3,\n",
       " 'are': 4,\n",
       " 'learning': 5,\n",
       " 'tokenization': 6,\n",
       " '34': 7,\n",
       " 'next': 8,\n",
       " 'will': 9,\n",
       " 'learn': 10,\n",
       " 'then': 11,\n",
       " 'stemming': 12,\n",
       " 'and': 13,\n",
       " 'lemmetization': 14}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=[\"We are learning tokenization 34\",\n",
    "           \"next we will learn sequencing 5\",\n",
    "           \"then Sequencing , Stemming and Lemmetization 5\"]\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "715eee07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'we': 1,\n",
       " 'sequencing': 2,\n",
       " 'are': 3,\n",
       " 'learning': 4,\n",
       " 'tokenization': 5,\n",
       " 'next': 6,\n",
       " 'will': 7,\n",
       " 'learn': 8,\n",
       " 'then': 9,\n",
       " 'stemming': 10,\n",
       " 'and': 11,\n",
       " 'lemmetization': 12}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=[\"We are learning tokenization 34\",\n",
    "           \"next we will learn sequencing 5\",\n",
    "           \"then Sequencing , Stemming and Lemmetization 5\"]\n",
    "\n",
    "tokenizer=Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a80c640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'we': 1,\n",
       " 'sequencing': 2,\n",
       " 'are': 3,\n",
       " 'learning': 4,\n",
       " 'tokenization': 5,\n",
       " '34': 6,\n",
       " 'next': 7,\n",
       " 'will': 8,\n",
       " 'learn': 9,\n",
       " \"'\": 10,\n",
       " 'then': 11,\n",
       " 'stemming': 12,\n",
       " 'and': 13,\n",
       " 'lemmetization': 14,\n",
       " '5': 15}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=[\"We are learning tokenization 34\",\n",
    "           \"next we will learn sequencing \",\n",
    "           \"then Sequencing , Stemming and Lemmetization 5\"]\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11275e90",
   "metadata": {},
   "source": [
    "### Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64251b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 1,\n",
       " 'refers': 2,\n",
       " 'representing': 3,\n",
       " 'a': 4,\n",
       " 'of': 5,\n",
       " 'we': 6,\n",
       " 'are': 7,\n",
       " 'learning': 8,\n",
       " 'text': 9,\n",
       " 'preprocessing': 10,\n",
       " 'tokenization': 11,\n",
       " 'each': 12,\n",
       " 'word': 13,\n",
       " 'with': 14,\n",
       " 'numerical': 15,\n",
       " 'token': 16,\n",
       " 'sequencing': 17,\n",
       " 'sentences': 18,\n",
       " 'as': 19,\n",
       " 'sequence': 20,\n",
       " 'tokens': 21,\n",
       " 'padding': 22,\n",
       " 'adding': 23,\n",
       " 'zeros': 24,\n",
       " 'make': 25,\n",
       " 'all': 26,\n",
       " 'sequences': 27,\n",
       " 'same': 28,\n",
       " 'length': 29}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['we are learning text preprocessing',\n",
    "            'Tokenization refers to representing each word with a numerical token',\n",
    "            'Sequencing refers to representing sentences as a sequence of tokens',\n",
    "            'padding refers to adding zeros to make all sequences of same length']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc07dfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 7, 8, 9, 10],\n",
       " [11, 2, 1, 3, 12, 13, 14, 4, 15, 16],\n",
       " [17, 2, 1, 3, 18, 19, 4, 20, 5, 21],\n",
       " [22, 2, 1, 23, 24, 1, 25, 26, 27, 5, 28, 29]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences=tokenizer.texts_to_sequences(sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b5d0496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 10, 11, 17, 22]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Text preprocessing involves tokenization ,sequencing, padding and more'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1f80a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 10, 11, 17, 22]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Text preprocessing does not involves tokenization ,sequencing, padding and more'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ec450",
   "metadata": {},
   "source": [
    "#### OOV(Out of Vocabulary ) Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22aa0c5",
   "metadata": {},
   "source": [
    "- Used to represent words which are not not in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36c428e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#OOV': 1,\n",
       " 'to': 2,\n",
       " 'refers': 3,\n",
       " 'representing': 4,\n",
       " 'a': 5,\n",
       " 'of': 6,\n",
       " 'we': 7,\n",
       " 'are': 8,\n",
       " 'learning': 9,\n",
       " 'text': 10,\n",
       " 'preprocessing': 11,\n",
       " 'tokenization': 12,\n",
       " 'each': 13,\n",
       " 'word': 14,\n",
       " 'with': 15,\n",
       " 'numerical': 16,\n",
       " 'token': 17,\n",
       " 'sequencing': 18,\n",
       " 'sentences': 19,\n",
       " 'as': 20,\n",
       " 'sequence': 21,\n",
       " 'tokens': 22,\n",
       " 'padding': 23,\n",
       " 'adding': 24,\n",
       " 'zeros': 25,\n",
       " 'make': 26,\n",
       " 'all': 27,\n",
       " 'sequences': 28,\n",
       " 'same': 29,\n",
       " 'length': 30}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['we are learning text preprocessing',\n",
    "            'Tokenization refers to representing each word with a numerical token',\n",
    "            'Sequencing refers to representing sentences as a sequence of tokens',\n",
    "            'padding refers to adding zeros to make all sequences of same length']\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='#OOV')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b696b8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 11, 1, 12, 18, 23, 1, 1]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Text preprocessing involves tokenization ,sequencing, padding and more'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b433124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 11, 1, 1, 1, 12, 18, 23, 1, 1]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Text preprocessing does not involves tokenization ,sequencing, padding and more'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2556c",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6331d3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 7, 5, 2],\n",
       " [3, 4, 2, 8],\n",
       " [3, 4, 2, 9],\n",
       " [3, 4, 2, 6, 10, 11, 12],\n",
       " [5, 2, 13, 14, 2, 4, 15],\n",
       " [3, 4, 16, 17, 18, 19, 6, 20, 21]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['We love machine learning',\n",
    "             'We are learning tokenization',\n",
    "             'we are learning sequencing',\n",
    "             'We are learning the cocept of padding',\n",
    "             'Machine learning and deep learning are fun',\n",
    "             'We are fortunate to learn from the best trainer']\n",
    "\n",
    "tokenizer = Tokenizer(oov_token = '#OOV')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7b735ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  3,  7,  5,  2],\n",
       "       [ 0,  0,  0,  0,  0,  3,  4,  2,  8],\n",
       "       [ 0,  0,  0,  0,  0,  3,  4,  2,  9],\n",
       "       [ 0,  0,  3,  4,  2,  6, 10, 11, 12],\n",
       "       [ 0,  0,  5,  2, 13, 14,  2,  4, 15],\n",
       "       [ 3,  4, 16, 17, 18, 19,  6, 20, 21]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_seq = pad_sequences(sequences)\n",
    "padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c7b47a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  7,  5,  2,  0,  0,  0,  0,  0],\n",
       "       [ 3,  4,  2,  8,  0,  0,  0,  0,  0],\n",
       "       [ 3,  4,  2,  9,  0,  0,  0,  0,  0],\n",
       "       [ 3,  4,  2,  6, 10, 11, 12,  0,  0],\n",
       "       [ 5,  2, 13, 14,  2,  4, 15,  0,  0],\n",
       "       [ 3,  4, 16, 17, 18, 19,  6, 20, 21]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq = pad_sequences(sequences,padding=\"post\") # or use 'pre'\n",
    "padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9b0f76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  9,  6,  2],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  4,  2, 10],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  4,  2, 11],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  3,  4,  2,  5, 12, 13, 14],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  6,  2, 15, 16,  2,  4, 17],\n",
       "       [ 0,  0,  0,  0,  0,  3,  4, 18,  7, 19, 20,  5, 21, 22],\n",
       "       [ 5, 23, 24, 25,  8, 26, 27,  7, 28,  8, 29, 30, 31, 32]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['We love machine learning',\n",
    "             'We are learning tokenization',\n",
    "             'we are learning sequencing',\n",
    "             'We are learning the cocept of padding',\n",
    "             'Machine learning and deep learning are fun',\n",
    "             'We are fortunate to learn from the best trainer',\n",
    "             'The main goal behing text preprocessing is to represent text in a numerical format']\n",
    "\n",
    "tokenizer = Tokenizer(oov_token = '#OOV')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_seq = pad_sequences(sequences, padding = 'pre')\n",
    "padded_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b13a9e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  3,  9,  6,  2],\n",
       "       [ 0,  0,  0,  0,  3,  4,  2, 10],\n",
       "       [ 0,  0,  0,  0,  3,  4,  2, 11],\n",
       "       [ 0,  3,  4,  2,  5, 12, 13, 14],\n",
       "       [ 0,  6,  2, 15, 16,  2,  4, 17],\n",
       "       [ 4, 18,  7, 19, 20,  5, 21, 22],\n",
       "       [27,  7, 28,  8, 29, 30, 31, 32]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token = '#OOV')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_seq = pad_sequences(sequences, padding = 'pre', maxlen = 8)\n",
    "padded_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edde58e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  3,  9,  6,  2],\n",
       "       [ 0,  0,  0,  0,  3,  4,  2, 10],\n",
       "       [ 0,  0,  0,  0,  3,  4,  2, 11],\n",
       "       [ 0,  3,  4,  2,  5, 12, 13, 14],\n",
       "       [ 0,  6,  2, 15, 16,  2,  4, 17],\n",
       "       [ 3,  4, 18,  7, 19, 20,  5, 21],\n",
       "       [ 5, 23, 24, 25,  8, 26, 27,  7]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token = '#OOV')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_seq = pad_sequences(sequences, padding = 'pre', maxlen = 8,truncating='post')\n",
    "padded_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54391093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  9,  6,  2,  0,  0,  0,  0],\n",
       "       [ 3,  4,  2, 10,  0,  0,  0,  0],\n",
       "       [ 3,  4,  2, 11,  0,  0,  0,  0],\n",
       "       [ 3,  4,  2,  5, 12, 13, 14,  0],\n",
       "       [ 6,  2, 15, 16,  2,  4, 17,  0],\n",
       "       [ 3,  4, 18,  7, 19, 20,  5, 21],\n",
       "       [ 5, 23, 24, 25,  8, 26, 27,  7]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token = '#OOV')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_seq = pad_sequences(sequences, padding = 'post', maxlen = 8,truncating='post')\n",
    "padded_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074bdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
