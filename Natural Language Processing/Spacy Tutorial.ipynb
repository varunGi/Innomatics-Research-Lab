{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14879575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\varshith\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (8.1.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\varshith\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\varshith\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.1)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.9.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\varshith\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a6152d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.4.1) (3.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.20.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.26.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.9.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (58.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.62.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\varshith\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\varshith\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.1)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.9.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\varshith\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\varshith\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.4.0\n",
      "    Uninstalling en-core-web-sm-3.4.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.4.0\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 12:35:47.435755: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-01-03 12:35:47.435825: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-03 12:35:57.189305: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2023-01-03 12:35:57.189356: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-03 12:35:57.205541: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-6DESMA2\n",
      "2023-01-03 12:35:57.205692: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-6DESMA2\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf478a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed669109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bedc9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "We are learning spacy"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1=nlp(\"We are learning spacy\")\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bcbb769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b54c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Guys,\\n\\nWe are learning spaCy , a cool nlp library.\\n\\nSome of its Features are:-\\n\\nEasy deep learning integration.\\nNon-destructive tokenization.\\nExport to numpy data arrays.\\nNamed entity recognition.\\nSupport for 51+ languages.\\nPre-trained word vectors.\\nState-of-the-art speed.\\nPart-of-speech tagging.\\nRobust, rigorously evaluated accuracy and many more'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a file\n",
    "myfile = open('learnspacy.txt').read()\n",
    "myfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd8180cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello Guys,\n",
       "\n",
       "We are learning spaCy , a cool nlp library.\n",
       "\n",
       "Some of its Features are:-\n",
       "\n",
       "Easy deep learning integration.\n",
       "Non-destructive tokenization.\n",
       "Export to numpy data arrays.\n",
       "Named entity recognition.\n",
       "Support for 51+ languages.\n",
       "Pre-trained word vectors.\n",
       "State-of-the-art speed.\n",
       "Part-of-speech tagging.\n",
       "Robust, rigorously evaluated accuracy and many more"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = nlp(myfile)\n",
    "file1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ecab99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d16539",
   "metadata": {},
   "source": [
    "### Setence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7419bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Hello Guys,\n",
      "\n",
      "We are learning spaCy , a cool nlp library.\n",
      "\n",
      "\n",
      "1 : Some of its Features are:-\n",
      "\n",
      "Easy deep learning integration.\n",
      "\n",
      "2 : Non-destructive tokenization.\n",
      "\n",
      "3 : Export to numpy data arrays.\n",
      "\n",
      "4 : Named entity recognition.\n",
      "\n",
      "5 : Support for 51+ languages.\n",
      "\n",
      "6 : Pre-trained word vectors.\n",
      "\n",
      "7 : State-of-the-art speed.\n",
      "\n",
      "8 : Part-of-speech tagging.\n",
      "\n",
      "9 : Robust, rigorously evaluated accuracy and many more\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(file1.sents):\n",
    "    print('{} : {}'.format(num, sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3076a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We love machine learning.\n",
      "We are learning tokenization.\n",
      "we are learning sequencing.\n",
      "We will learn padding.\n",
      "Machine learning and deep learning are fun.\n",
      "We are fortunate to learn from the best trainer.\n",
      "We need sequence models to deal with sequence data , examples are RNN, LSTM, GRU.\n"
     ]
    }
   ],
   "source": [
    "sentences = ['We love machine learning.',\n",
    "             'We are learning tokenization.',\n",
    "             'we are learning sequencing.',\n",
    "             'We will learn padding.',\n",
    "             'Machine learning and deep learning are fun.',\n",
    "             'We are fortunate to learn from the best trainer.',\n",
    "             'We need sequence models to deal with sequence data , examples are RNN, LSTM, GRU.']\n",
    "\n",
    "sentences = ' '.join(sentences)\n",
    "\n",
    "doc3 = nlp(sentences)\n",
    "for sentence in doc3.sents:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26866d",
   "metadata": {},
   "source": [
    "### Word  Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc0adcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "We are learning spacy"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbc20d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "are\n",
      "learning\n",
      "spacy\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99adc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I True\n",
      "have True\n",
      "3 False\n",
      "coins True\n",
      "and True\n",
      "a True\n",
      "10 False\n",
      "rupee True\n",
      "note True\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp('I have 3 coins and a 10 rupee note')\n",
    "doc2\n",
    "for word in doc2:\n",
    "    print(word.text, word.is_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6a3c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We     PRON\n",
      "are     AUX\n",
      "learning     VERB\n",
      "spacy     NOUN\n"
     ]
    }
   ],
   "source": [
    "for word in doc1: \n",
    "    print(word.text, '   ', word.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9996cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON PRP\n",
      "like AUX VBP\n",
      "listening VERB VBG\n",
      "to ADP IN\n",
      "music NOUN NN\n",
      "while SCONJ IN\n",
      "reading VERB VBG\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp('I like listening to music while reading')\n",
    "for word in doc3: \n",
    "    print(word.text, word.pos_, word.tag_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3ef3aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON', 'PRP'),\n",
       " ('like', 'AUX', 'VBP'),\n",
       " ('listening', 'VERB', 'VBG'),\n",
       " ('to', 'ADP', 'IN'),\n",
       " ('music', 'NOUN', 'NN'),\n",
       " ('while', 'SCONJ', 'IN'),\n",
       " ('reading', 'VERB', 'VBG')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagged_words = [(word.text, word.pos_, word.tag_) for word in doc3]\n",
    "pos_tagged_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1721c6b",
   "metadata": {},
   "source": [
    "### Visual dependency using displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb8cdff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47e8f638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e626b7dcbea54d289a2a183feeda8943-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">We</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Spacy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e626b7dcbea54d289a2a183feeda8943-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e626b7dcbea54d289a2a183feeda8943-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e626b7dcbea54d289a2a183feeda8943-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e626b7dcbea54d289a2a183feeda8943-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e626b7dcbea54d289a2a183feeda8943-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e626b7dcbea54d289a2a183feeda8943-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc1=nlp(\"We are learning Spacy\")\n",
    "displacy.render(doc1,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a729ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unclassified dependent'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97dcc404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marker'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('mark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae8ce35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'object of preposition'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('pobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58623066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b9f842d",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2469b8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I\n",
      "am be\n",
      "playing play\n",
      "football football\n",
      "i I\n",
      "have have\n",
      "played play\n",
      "a a\n",
      "lot lot\n",
      "i I\n",
      "am be\n",
      "a a\n",
      "good good\n",
      "player player\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp('I am playing football i have played a lot i am a good player')\n",
    "for word in doc3:\n",
    "    print(word.text, word.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc77f5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'be',\n",
       " 'play',\n",
       " 'football',\n",
       " 'I',\n",
       " 'have',\n",
       " 'play',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'I',\n",
       " 'be',\n",
       " 'a',\n",
       " 'good',\n",
       " 'player']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words = [word.lemma_ for word in doc3]\n",
    "lemmatized_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec1019",
   "metadata": {},
   "source": [
    "### Named Entity Regcognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95982d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2025, India, Technical, more than 5 billion dollars)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc5 = nlp('By 2025, India will grow so much in Technical field and earn more than 5 billion dollars')\n",
    "doc5.ents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03468a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2025, India, Technical, more than 5 billion dollars)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc5 = nlp('By 2025, India will grow so much in Technical field and earn more than 5 billion dollars')\n",
    "doc5.ents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "369ade80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025 DATE\n",
      "India GPE\n",
      "Technical ORG\n",
      "more than 5 billion dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "for word in doc5.ents:\n",
    "    print(word.text, word.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "950b7521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd8b82a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">By \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2025\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " will grow so much in \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Technical\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " field and earn \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    more than 5 billion dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc5, style = 'ent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9662e20",
   "metadata": {},
   "source": [
    "### Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "848e0021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varshith\\AppData\\Local\\Temp\\ipykernel_12420\\962163034.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  word1.similarity(word2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.560428581947586"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1=nlp('horse')\n",
    "word2=nlp('elephant')\n",
    "word1.similarity(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74490120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varshith\\AppData\\Local\\Temp\\ipykernel_12420\\404527184.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  word1.similarity(word2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.694920692451447"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1=nlp('tiger')\n",
    "word2=nlp('lion')\n",
    "word1.similarity(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9fa6bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varshith\\AppData\\Local\\Temp\\ipykernel_12420\\2458996283.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  word1.similarity(word2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7940567466319468"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1=nlp('happy')\n",
    "word2=nlp('glad')\n",
    "word1.similarity(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b5ecd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Happy', 'Happy') Similarity:- 1.0\n",
      "('Happy', 'and') Similarity:- -0.09705375880002975\n",
      "('Happy', 'glad') Similarity:- 0.29409945011138916\n",
      "('Happy', 'to') Similarity:- 0.01984683983027935\n",
      "('Happy', 'have') Similarity:- 0.004240707959979773\n",
      "('Happy', 'a') Similarity:- 0.056658972054719925\n",
      "('Happy', 'horse') Similarity:- -0.044781941920518875\n",
      "('Happy', ',') Similarity:- 0.021253032609820366\n",
      "('Happy', 'cat') Similarity:- 0.27863609790802\n",
      "('Happy', 'and') Similarity:- -0.05193940922617912\n",
      "('Happy', 'fish') Similarity:- -0.08898986130952835\n",
      "('and', 'Happy') Similarity:- -0.09705375880002975\n",
      "('and', 'and') Similarity:- 1.0\n",
      "('and', 'glad') Similarity:- -0.0016224179416894913\n",
      "('and', 'to') Similarity:- 0.11946848779916763\n",
      "('and', 'have') Similarity:- -0.040635377168655396\n",
      "('and', 'a') Similarity:- -0.10031401365995407\n",
      "('and', 'horse') Similarity:- -0.1967398077249527\n",
      "('and', ',') Similarity:- 0.07385743409395218\n",
      "('and', 'cat') Similarity:- -0.17075084149837494\n",
      "('and', 'and') Similarity:- 1.0\n",
      "('and', 'fish') Similarity:- 0.001340966671705246\n",
      "('glad', 'Happy') Similarity:- 0.29409945011138916\n",
      "('glad', 'and') Similarity:- -0.0016224179416894913\n",
      "('glad', 'glad') Similarity:- 1.0\n",
      "('glad', 'to') Similarity:- 0.08686356991529465\n",
      "('glad', 'have') Similarity:- 0.04873039573431015\n",
      "('glad', 'a') Similarity:- -0.10346296429634094\n",
      "('glad', 'horse') Similarity:- 0.050097379833459854\n",
      "('glad', ',') Similarity:- 0.0192404817789793\n",
      "('glad', 'cat') Similarity:- 0.10583838075399399\n",
      "('glad', 'and') Similarity:- 0.06299278140068054\n",
      "('glad', 'fish') Similarity:- 0.325870156288147\n",
      "('to', 'Happy') Similarity:- 0.01984683983027935\n",
      "('to', 'and') Similarity:- 0.11946848779916763\n",
      "('to', 'glad') Similarity:- 0.08686356991529465\n",
      "('to', 'to') Similarity:- 1.0\n",
      "('to', 'have') Similarity:- 0.017950724810361862\n",
      "('to', 'a') Similarity:- 0.081868976354599\n",
      "('to', 'horse') Similarity:- 0.031032036989927292\n",
      "('to', ',') Similarity:- -0.056445855647325516\n",
      "('to', 'cat') Similarity:- -0.036750175058841705\n",
      "('to', 'and') Similarity:- 0.05090251564979553\n",
      "('to', 'fish') Similarity:- 0.037268318235874176\n",
      "('have', 'Happy') Similarity:- 0.004240707959979773\n",
      "('have', 'and') Similarity:- -0.040635377168655396\n",
      "('have', 'glad') Similarity:- 0.04873039573431015\n",
      "('have', 'to') Similarity:- 0.017950724810361862\n",
      "('have', 'have') Similarity:- 1.0\n",
      "('have', 'a') Similarity:- -0.02088732272386551\n",
      "('have', 'horse') Similarity:- 0.18652953207492828\n",
      "('have', ',') Similarity:- -0.11767645180225372\n",
      "('have', 'cat') Similarity:- 0.17203788459300995\n",
      "('have', 'and') Similarity:- -0.10331767052412033\n",
      "('have', 'fish') Similarity:- 0.2931971251964569\n",
      "('a', 'Happy') Similarity:- 0.056658972054719925\n",
      "('a', 'and') Similarity:- -0.10031401365995407\n",
      "('a', 'glad') Similarity:- -0.10346296429634094\n",
      "('a', 'to') Similarity:- 0.081868976354599\n",
      "('a', 'have') Similarity:- -0.02088732272386551\n",
      "('a', 'a') Similarity:- 1.0\n",
      "('a', 'horse') Similarity:- 0.07767900079488754\n",
      "('a', ',') Similarity:- -0.11902071535587311\n",
      "('a', 'cat') Similarity:- -0.02542492188513279\n",
      "('a', 'and') Similarity:- -0.01734035275876522\n",
      "('a', 'fish') Similarity:- -0.11956655234098434\n",
      "('horse', 'Happy') Similarity:- -0.044781941920518875\n",
      "('horse', 'and') Similarity:- -0.1967398077249527\n",
      "('horse', 'glad') Similarity:- 0.050097379833459854\n",
      "('horse', 'to') Similarity:- 0.031032036989927292\n",
      "('horse', 'have') Similarity:- 0.18652953207492828\n",
      "('horse', 'a') Similarity:- 0.07767900079488754\n",
      "('horse', 'horse') Similarity:- 1.0\n",
      "('horse', ',') Similarity:- -0.009672176092863083\n",
      "('horse', 'cat') Similarity:- 0.2764114439487457\n",
      "('horse', 'and') Similarity:- -0.1836555153131485\n",
      "('horse', 'fish') Similarity:- 0.2520979940891266\n",
      "(',', 'Happy') Similarity:- 0.021253032609820366\n",
      "(',', 'and') Similarity:- 0.07385743409395218\n",
      "(',', 'glad') Similarity:- 0.0192404817789793\n",
      "(',', 'to') Similarity:- -0.056445855647325516\n",
      "(',', 'have') Similarity:- -0.11767645180225372\n",
      "(',', 'a') Similarity:- -0.11902071535587311\n",
      "(',', 'horse') Similarity:- -0.009672176092863083\n",
      "(',', ',') Similarity:- 1.0\n",
      "(',', 'cat') Similarity:- 0.022681426256895065\n",
      "(',', 'and') Similarity:- 0.14183324575424194\n",
      "(',', 'fish') Similarity:- -0.05460663512349129\n",
      "('cat', 'Happy') Similarity:- 0.27863609790802\n",
      "('cat', 'and') Similarity:- -0.17075084149837494\n",
      "('cat', 'glad') Similarity:- 0.10583838075399399\n",
      "('cat', 'to') Similarity:- -0.036750175058841705\n",
      "('cat', 'have') Similarity:- 0.17203788459300995\n",
      "('cat', 'a') Similarity:- -0.02542492188513279\n",
      "('cat', 'horse') Similarity:- 0.2764114439487457\n",
      "('cat', ',') Similarity:- 0.022681426256895065\n",
      "('cat', 'cat') Similarity:- 1.0\n",
      "('cat', 'and') Similarity:- -0.09366057068109512\n",
      "('cat', 'fish') Similarity:- 0.27168193459510803\n",
      "('and', 'Happy') Similarity:- -0.05193940922617912\n",
      "('and', 'and') Similarity:- 1.0\n",
      "('and', 'glad') Similarity:- 0.06299278140068054\n",
      "('and', 'to') Similarity:- 0.05090251564979553\n",
      "('and', 'have') Similarity:- -0.10331767052412033\n",
      "('and', 'a') Similarity:- -0.01734035275876522\n",
      "('and', 'horse') Similarity:- -0.1836555153131485\n",
      "('and', ',') Similarity:- 0.14183324575424194\n",
      "('and', 'cat') Similarity:- -0.09366057068109512\n",
      "('and', 'and') Similarity:- 1.0\n",
      "('and', 'fish') Similarity:- 0.07007904350757599\n",
      "('fish', 'Happy') Similarity:- -0.08898986130952835\n",
      "('fish', 'and') Similarity:- 0.001340966671705246\n",
      "('fish', 'glad') Similarity:- 0.325870156288147\n",
      "('fish', 'to') Similarity:- 0.037268318235874176\n",
      "('fish', 'have') Similarity:- 0.2931971251964569\n",
      "('fish', 'a') Similarity:- -0.11956655234098434\n",
      "('fish', 'horse') Similarity:- 0.2520979940891266\n",
      "('fish', ',') Similarity:- -0.05460663512349129\n",
      "('fish', 'cat') Similarity:- 0.27168193459510803\n",
      "('fish', 'and') Similarity:- 0.07007904350757599\n",
      "('fish', 'fish') Similarity:- 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varshith\\AppData\\Local\\Temp\\ipykernel_12420\\1189197761.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print((w1.text,w2.text),\"Similarity:-\",w1.similarity(w2))\n"
     ]
    }
   ],
   "source": [
    "doc6=nlp(\"Happy and glad to have a horse ,cat and fish\")\n",
    "for w1 in doc6:\n",
    "    for w2 in doc6:\n",
    "        print((w1.text,w2.text),\"Similarity:-\",w1.similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821c754",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "098eb932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'most', 'latter', 'toward', 'whither', 'four', 'everywhere', 'three', 'eleven', 'made', 'other', 'whereupon', \"n't\", 'neither', 'almost', 'take', 'enough', 'ours', 'yourselves', 'throughout', 'your', \"'ve\", 'so', 'to', 'than', 'me', '‘s', 'these', 'do', 'another', 'only', 'five', 'onto', 'thru', 'via', 'my', 'others', 'while', 'with', 'empty', 'may', 'regarding', 'until', \"'re\", 'here', 'due', 'does', 'nor', 'anyhow', 'something', 'even', 'over', 'various', 'an', 'ourselves', 'go', 'are', 'what', 'quite', 'elsewhere', 'against', 'several', 'their', 'had', 'thereupon', 'then', 'together', 'herself', 'nobody', 'that', 'therein', 'down', 'must', 'more', 'him', 'always', 'per', 'hereafter', 'among', 'now', '‘ll', 'ever', 'would', 'can', 'forty', 'next', 'might', 'thereafter', 'few', 'unless', 'became', 'out', 'why', 'however', 'mostly', 'thereby', 'will', 'part', 'above', 'just', 'make', 'without', 'whether', 'should', 'anywhere', 'he', 'beside', 'seems', 'its', 'third', 'everything', 'eight', 'whereby', 'seem', 'both', 'yourself', 'nine', '’s', 'whereafter', 'less', 'whoever', 'when', 'has', 'along', 'hereby', 'never', 'where', 'still', 'wherein', 'his', 'below', 'around', 'herein', 'our', 'anyone', 'hence', 'whole', 'please', 'serious', 'say', 'last', 'those', 'mine', 'am', 'used', 'fifteen', 'already', \"'d\", 'she', 'whatever', 'somehow', 'such', 'twelve', 'about', 'be', 'under', 'bottom', 'because', 'ten', 'move', 'it', \"'s\", '’ve', 'much', 'in', 'of', 'cannot', '‘m', 'a', 'nevertheless', 'there', 'being', '‘re', 'thus', 'this', 'meanwhile', 'who', 'once', 'is', 'show', 'sometimes', 'across', 'everyone', 'moreover', 'except', 'the', 'upon', 'amount', 'were', 'on', 'done', 'often', 'from', 'or', 'someone', '’m', 'own', 'doing', 'yours', 'during', 'name', 'too', 'hereupon', \"'ll\", 'n‘t', 'six', 'us', 'afterwards', 'her', 'full', 'anything', 'some', 'was', 'for', 'former', 'since', 'wherever', 'therefore', 'side', 'as', 'becoming', 'after', 'front', 'between', 'before', 'could', '’d', 'somewhere', 'by', 'first', 'nothing', 'alone', 'which', 'sixty', 'himself', 'all', 'hundred', 'every', 'n’t', '‘ve', 'off', 'really', 'same', 'twenty', 'if', 'did', 'into', 'get', 'become', 'i', 'put', 're', 'namely', 'least', 'top', 'becomes', 'they', 'again', 'no', 'beforehand', 'not', 'hers', 'seeming', 'either', 'amongst', 'give', 'within', 'whose', 'you', 'back', 'noone', 'latterly', 'fifty', 'any', 'seemed', '’re', 'myself', 'whom', 'yet', 'sometime', 'very', 'up', 'whereas', 'rather', '‘d', 'see', \"'m\", 'keep', 'themselves', 'though', 'also', 'whenever', 'and', 'itself', 'many', 'anyway', 'formerly', 'whence', 'but', '’ll', 'two', 'nowhere', 'through', 'them', 'been', 'further', 'ca', 'how', 'although', 'one', 'well', 'each', 'besides', 'we', 'call', 'none', 'using', 'thence', 'perhaps', 'towards', 'have', 'behind', 'at', 'otherwise', 'else', 'indeed', 'beyond'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588770fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
